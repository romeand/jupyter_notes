{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. El conjunto de datos game_purchases.csv contiene datos de compras dentro de la aplicación para el juego de estrategia Battle Universe. Las columnas en la tabla son:\n",
    "\n",
    "purchase_datetime: la fecha y el tiempo de la compra.\n",
    "player_id: el identificador único del jugador.\n",
    "item: el nombre del artículo comprado: life, mana (poder mágico utilizado para lanzar hechizos), armor.\n",
    "price (en la divisa del juego).\n",
    "purchase_id: el identificador único de la compra.\n",
    "Lee los datos del archivo game_purchases.csv. Guarda el resultado como purchases.\n",
    "\n",
    "Agrupa los datos de la tabla purchases: encuentra la fecha de la primera compra para cada jugador. Nombra al objeto Series resultante first_purchase_dates. \n",
    "\n",
    "Después cambia el nombre del objeto Series first_purchase_datetime cambiando el atributo name. \n",
    "\n",
    "Imprime las primeras 10 filas de first_purchase_dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "purchases = pd.read_csv('/datasets/game_purchases.csv')\n",
    "\n",
    "first_purchase_dates= purchases.groupby('player_id')['purchase_datetime'].min()\n",
    "first_purchase_dates.name = 'first_purchase_datetime'\n",
    "print(first_purchase_dates.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "player_id\n",
    "00065bd9-5355-4c4b-b331-e8dcb74c18dc    2019-03-16 12:34:54\n",
    "00209bef-ccb4-44d1-b385-7a7a006b81a9    2019-01-06 01:51:41\n",
    "0024571d-3a4a-4664-858c-ac4317205892    2019-05-23 03:13:15\n",
    "00369580-9fbf-4f2f-b3bf-60b18aacdb32    2019-03-04 19:05:45\n",
    "003a9684-be82-4b7e-a176-4cc8a6beb6fe    2019-01-14 21:48:20\n",
    "004b5aaa-1881-49e4-8d1c-79182f7181cb    2019-01-14 11:08:05\n",
    "00557d31-77ed-4f77-90e7-99b0cb7be674    2019-01-14 10:32:20\n",
    "00658dc0-ee2d-4831-9c86-9c9ebdaa0ccc    2019-01-07 22:47:55\n",
    "00959130-8b4a-4ba0-8a7d-29b3469e5a1d    2019-01-23 11:13:10\n",
    "009fa280-fe17-40f7-868c-0a10394addb3    2019-04-22 08:21:04\n",
    "Name: first_purchase_datetime, dtype: object\n",
    "\n",
    "2.Une el DataFrame purchases con el objeto Series first_purchase_dates para que first_purchase_dates se convierta en una columna separada en el DataFrame purchases.  \n",
    "\n",
    "Guarda el DataFrame conjunto como purchases. \n",
    "\n",
    "Imprime las 10 primeras filas del DataFrame conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "purchases = pd.read_csv('/datasets/game_purchases.csv')\n",
    "first_purchase_dates = purchases.groupby('player_id')[\n",
    "    'purchase_datetime'\n",
    "].min()\n",
    "first_purchase_dates.name = 'first_purchase_datetime'\n",
    "purchases = purchases.join(first_purchase_dates,on='player_id')\n",
    "print(purchases.head(10)) # Imprime las 10 primeras filas del DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "     purchase_datetime  ... first_purchase_datetime\n",
    "0  2019-01-01 00:15:54  ...     2019-01-01 00:15:54\n",
    "1  2019-01-01 00:16:56  ...     2019-01-01 00:16:56\n",
    "2  2019-01-01 00:56:17  ...     2019-01-01 00:56:17\n",
    "3  2019-01-01 00:58:17  ...     2019-01-01 00:58:17\n",
    "4  2019-01-01 01:15:42  ...     2019-01-01 01:15:42\n",
    "5  2019-01-01 01:16:46  ...     2019-01-01 01:16:46\n",
    "6  2019-01-01 01:24:40  ...     2019-01-01 01:24:40\n",
    "7  2019-01-01 01:43:49  ...     2019-01-01 01:43:49\n",
    "8  2019-01-01 01:48:47  ...     2019-01-01 01:48:47\n",
    "9  2019-01-01 01:51:21  ...     2019-01-01 01:51:21\n",
    "\n",
    "[10 rows x 6 columns]\n",
    "\n",
    "3.En el DataFrame purchases, crea una columna purchase_month para almacenar los meses recuperados de la columna purchase_datetime. \n",
    "\n",
    "También crea una columna first_purchase_month donde almacenarás los meses recuperados de la columna first_purchase_datetime. Esta columna se utilizará para crear cohortes.\n",
    "\n",
    "Imprime las 10 primeras filas del DataFrame purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "purchases = pd.read_csv('/datasets/game_purchases.csv')\n",
    "first_purchase_dates = purchases.groupby('player_id')[\n",
    "    'purchase_datetime'\n",
    "].min()\n",
    "first_purchase_dates.name = 'first_purchase_datetime'\n",
    "purchases = purchases.join(first_purchase_dates, on='player_id')\n",
    "purchases['purchase_month'] = purchases['purchase_datetime'].astype('datetime64[M]')\n",
    "purchases['first_purchase_month'] = purchases['first_purchase_datetime'].astype('datetime64[M]')\n",
    "print(purchases.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "     purchase_datetime  ... first_purchase_month\n",
    "0  2019-01-01 00:15:54  ...           2019-01-01\n",
    "1  2019-01-01 00:16:56  ...           2019-01-01\n",
    "2  2019-01-01 00:56:17  ...           2019-01-01\n",
    "3  2019-01-01 00:58:17  ...           2019-01-01\n",
    "4  2019-01-01 01:15:42  ...           2019-01-01\n",
    "5  2019-01-01 01:16:46  ...           2019-01-01\n",
    "6  2019-01-01 01:24:40  ...           2019-01-01\n",
    "7  2019-01-01 01:43:49  ...           2019-01-01\n",
    "8  2019-01-01 01:48:47  ...           2019-01-01\n",
    "9  2019-01-01 01:51:21  ...           2019-01-01\n",
    "\n",
    "[10 rows x 8 columns]\n",
    "\n",
    "4.Haz una tabla con el DataFrame de purchases donde calcularás la suma total gastada en compras dentro del juego para cada cohorte.\n",
    "\n",
    "Guarda la tabla agrupada como cohort_stats.\n",
    "\n",
    "Muestra cohort_stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "purchases = pd.read_csv('/datasets/game_purchases.csv')\n",
    "first_purchase_dates = purchases.groupby('player_id')[\n",
    "    'purchase_datetime'\n",
    "].min()\n",
    "first_purchase_dates.name = 'first_purchase_datetime'\n",
    "purchases = purchases.join(first_purchase_dates, on='player_id')\n",
    "purchases['purchase_month'] = purchases['purchase_datetime'].astype(\n",
    "    'datetime64[M]'\n",
    ")\n",
    "purchases['first_purchase_month'] = purchases[\n",
    "    'first_purchase_datetime'\n",
    "].astype('datetime64[M]')\n",
    "cohort_stats = purchases.groupby('first_purchase_month')['price'].sum()\n",
    "print(cohort_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "first_purchase_month\n",
    "2019-01-01    2432.2\n",
    "2019-02-01     602.5\n",
    "2019-03-01     292.7\n",
    "2019-04-01     192.8\n",
    "2019-05-01     108.5\n",
    "2019-06-01      72.8\n",
    "2019-07-01      46.7\n",
    "Name: price, dtype: float64\n",
    "\n",
    "EVALUACIÓN DE CAMBIOS EN LOS VALORES ABSOLUTOS POR MES\n",
    "\n",
    "1.Compila una tabla dinámica para mostrar los cambios en las compras totales realizadas por los jugadores en cohortes mensuales. Guárdala como purchase_pivot. \n",
    "\n",
    "Muestra la tabla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "purchases = pd.read_csv('/datasets/game_purchases.csv')\n",
    "first_purchase_dates = purchases.groupby('player_id')[\n",
    "    'purchase_datetime'\n",
    "].min()\n",
    "first_purchase_dates.name = 'first_purchase_datetime'\n",
    "purchases = purchases.join(first_purchase_dates, on='player_id')\n",
    "purchases['purchase_month'] = purchases['purchase_datetime'].astype(\n",
    "    'datetime64[M]'\n",
    ")\n",
    "purchases['first_purchase_month'] = purchases[\n",
    "    'first_purchase_datetime'\n",
    "].astype('datetime64[M]')\n",
    "purchase_pivot = purchases.pivot_table(index='first_purchase_month', columns='purchase_month',values='price',aggfunc='sum')\n",
    "print(purchase_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "purchase_month        2019-01-01  2019-02-01  2019-03-01  2019-04-01  2019-05-01  2019-06-01  2019-07-01\n",
    "first_purchase_month                                                                                    \n",
    "2019-01-01                 555.6       303.7       318.2       323.3       314.3       313.1       304.0\n",
    "2019-02-01                   NaN       218.9        77.1        75.8        81.2        69.3        80.2\n",
    "2019-03-01                   NaN         NaN       145.1        35.7        38.8        38.1        35.0\n",
    "2019-04-01                   NaN         NaN         NaN       111.5        28.9        23.6        28.8\n",
    "2019-05-01                   NaN         NaN         NaN         NaN        81.8        12.9        13.8\n",
    "2019-06-01                   NaN         NaN         NaN         NaN         NaN        61.5        11.3\n",
    "2019-07-01                   NaN         NaN         NaN         NaN         NaN         NaN        46.7\n",
    "\n",
    "2.Encuentra el número de compras y el número de jugadores que realizaron compras para cada cohorte y para cada mes. Guárdalo como purchases_grouped_by_cohorts. \n",
    "\n",
    "Agrega la columna purchases_per_player al DataFrame purchases_grouped_by_cohorts. En esta columna, calcula el promedio de compras por jugador.\n",
    "\n",
    "Compila una tabla dinámica y guárdala como mean_purchases_pivot. Contendrá cambios en el número promedio de compras por jugador en cada cohorte en diferentes meses.\n",
    "\n",
    "Imprime la tabla dinámica mean_purchases_pivot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "purchases = pd.read_csv('/datasets/game_purchases.csv')\n",
    "first_purchase_dates = purchases.groupby('player_id')[\n",
    "    'purchase_datetime'\n",
    "].min()\n",
    "first_purchase_dates.name = 'first_purchase_datetime'\n",
    "purchases = purchases.join(first_purchase_dates, on='player_id')\n",
    "purchases['purchase_month'] = purchases['purchase_datetime'].astype(\n",
    "    'datetime64[M]'\n",
    ")\n",
    "purchases['first_purchase_month'] = purchases[\n",
    "    'first_purchase_datetime'\n",
    "].astype('datetime64[M]')\n",
    "purchases_grouped_by_cohorts = purchases.groupby(['first_purchase_month','purchase_month']).agg({'purchase_id':'nunique','player_id':'nunique'}) # Encuentra el número de compras y el número de jugadores que realizaron compras\n",
    "purchases_grouped_by_cohorts['purchases_per_player']=(purchases_grouped_by_cohorts['purchase_id']/purchases_grouped_by_cohorts['player_id'])\n",
    "# Crea una columna 'purchases_per_player'\n",
    "\n",
    "mean_purchases_pivot  = purchases_grouped_by_cohorts.pivot_table(\n",
    "    index='first_purchase_month',\n",
    "    columns='purchase_month',\n",
    "    values='purchases_per_player',\n",
    "    aggfunc='sum',)\n",
    "print(mean_purchases_pivot) # Muestra la tabla dinámica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "purchase_month        2019-01-01  2019-02-01  2019-03-01  2019-04-01  2019-05-01  2019-06-01  2019-07-01\n",
    "first_purchase_month                                                                                    \n",
    "2019-01-01              1.671333    2.045802    2.067055    2.177979    2.137462    2.000000    2.025602\n",
    "2019-02-01                   NaN    1.256881    1.456522    1.359833    1.361538    1.362832    1.384921\n",
    "2019-03-01                   NaN         NaN    1.175601    1.246032    1.297710    1.330769    1.288136\n",
    "2019-04-01                   NaN         NaN         NaN    1.149425    1.310680    1.219780    1.320388\n",
    "2019-05-01                   NaN         NaN         NaN         NaN    1.085366    1.145455    1.166667\n",
    "2019-06-01                   NaN         NaN         NaN         NaN         NaN    1.116183    1.170732\n",
    "2019-07-01                   NaN         NaN         NaN         NaN         NaN         NaN    1.072917\n",
    "\n",
    "EVALUACIÒN DE LOS CAMBIOS EN LOS VALORES RELATIVOS POR CICLO DE VIDA\n",
    "\n",
    "1.Continuaremos la serie de tareas de la lección anterior.\n",
    "\n",
    "Introduzcamos la columna cohort_lifetime en la tabla purchases_grouped_by_cohorts. Dentro de la columna, convierte el mes de compra en el juego (purchase_month) en el mes de ciclo de vida (el mes relativo al mes de la cohorte).\n",
    "\n",
    "La columna cohort_lifetime debe contener números enteros.\n",
    "\n",
    "Esta tarea da un resultado intermedio. No tienes que mostrar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "purchases = pd.read_csv('/datasets/game_purchases.csv')\n",
    "first_purchase_dates = purchases.groupby('player_id')[\n",
    "    'purchase_datetime'\n",
    "].min()\n",
    "first_purchase_dates.name = 'first_purchase_datetime'\n",
    "purchases = purchases.join(first_purchase_dates, on='player_id')\n",
    "purchases['purchase_month'] = purchases['purchase_datetime'].astype(\n",
    "    'datetime64[M]'\n",
    ")\n",
    "purchases['first_purchase_month'] = purchases[\n",
    "    'first_purchase_datetime'\n",
    "].astype('datetime64[M]')\n",
    "purchases_grouped_by_cohorts = purchases.groupby(\n",
    "    ['first_purchase_month', 'purchase_month']\n",
    ").agg({'purchase_id': 'nunique', 'player_id': 'nunique'})\n",
    "purchases_grouped_by_cohorts['purchases_per_player'] = (\n",
    "    purchases_grouped_by_cohorts['purchase_id']\n",
    "    / purchases_grouped_by_cohorts['player_id']\n",
    ")\n",
    "purchases_grouped_by_cohorts = purchases_grouped_by_cohorts.reset_index()\n",
    "purchases_grouped_by_cohorts['cohort_lifetime'] = (purchases_grouped_by_cohorts['purchase_month']-purchases_grouped_by_cohorts['first_purchase_month'])\n",
    "\n",
    "purchases_grouped_by_cohorts['cohort_lifetime'] = purchases_grouped_by_cohorts['cohort_lifetime']/np.timedelta64(1,'M')\n",
    "\n",
    "purchases_grouped_by_cohorts['cohort_lifetime'] = (purchases_grouped_by_cohorts['cohort_lifetime'].round().astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Compila una tabla dinámica para mostrar los cambios en el número promedio de compras por jugador en relación con el mes de ciclo de vida de la cohorte. Guarda la tabla como lifetime_pivot e imprime la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "purchases = pd.read_csv('/datasets/game_purchases.csv')\n",
    "first_purchase_dates = purchases.groupby('player_id')[\n",
    "    'purchase_datetime'\n",
    "].min()\n",
    "first_purchase_dates.name = 'first_purchase_datetime'\n",
    "purchases = purchases.join(first_purchase_dates, on='player_id')\n",
    "purchases['purchase_month'] = purchases['purchase_datetime'].astype(\n",
    "    'datetime64[M]'\n",
    ")\n",
    "purchases['first_purchase_month'] = purchases[\n",
    "    'first_purchase_datetime'\n",
    "].astype('datetime64[M]')\n",
    "purchases_grouped_by_cohorts = purchases.groupby(\n",
    "    ['first_purchase_month', 'purchase_month']\n",
    ").agg({'purchase_id': 'nunique', 'player_id': 'nunique'})\n",
    "purchases_grouped_by_cohorts['purchases_per_player'] = (\n",
    "    purchases_grouped_by_cohorts['purchase_id']\n",
    "    / purchases_grouped_by_cohorts['player_id']\n",
    ")\n",
    "purchases_grouped_by_cohorts = purchases_grouped_by_cohorts.reset_index()\n",
    "purchases_grouped_by_cohorts['cohort_lifetime'] = (\n",
    "    purchases_grouped_by_cohorts['purchase_month']\n",
    "    - purchases_grouped_by_cohorts['first_purchase_month']\n",
    ")\n",
    "purchases_grouped_by_cohorts['cohort_lifetime'] = purchases_grouped_by_cohorts[\n",
    "    'cohort_lifetime'\n",
    "] / np.timedelta64(1, 'M')\n",
    "purchases_grouped_by_cohorts['cohort_lifetime'] = (\n",
    "    purchases_grouped_by_cohorts['cohort_lifetime'].round().astype('int')\n",
    ")\n",
    "lifetime_pivot = purchases_grouped_by_cohorts.pivot_table(\n",
    "    index='first_purchase_month',\n",
    "    columns='cohort_lifetime',\n",
    "    values='purchases_per_player',\n",
    "    aggfunc='sum',\n",
    ")\n",
    "print(lifetime_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "cohort_lifetime              0         1         2         3         4         5         6\n",
    "first_purchase_month                                                                      \n",
    "2019-01-01            1.671333  2.045802  2.067055  2.177979  2.137462  2.000000  2.025602\n",
    "2019-02-01            1.256881  1.456522  1.359833  1.361538  1.362832  1.384921       NaN\n",
    "2019-03-01            1.175601  1.246032  1.297710  1.330769  1.288136       NaN       NaN\n",
    "2019-04-01            1.149425  1.310680  1.219780  1.320388       NaN       NaN       NaN\n",
    "2019-05-01            1.085366  1.145455  1.166667       NaN       NaN       NaN       NaN\n",
    "2019-06-01            1.116183  1.170732       NaN       NaN       NaN       NaN       NaN\n",
    "2019-07-01            1.072917       NaN       NaN       NaN       NaN       NaN       NaN\n",
    "\n",
    "VISUALIZACIÓN DE ANÁLISIS DE COHORTES\n",
    "\n",
    "En el DataFrame revenue_per_user_pivot puedes encontrar una tabla dinámica que refleja los cambios en el tamaño de compra promedio de los clientes de la tienda Sports Are Good por cohortes agrupadas por el ciclo de vida. Utiliza el método heatmap() para hacer un mapa de calor de los cambios en el tamaño de compra promedio de las cohortes.\n",
    "\n",
    "Establece el tamaño de la figura: plt.figure(figsize=(13, 9)). Asigna a la visualización resultante el nombre 'Tamaño promedio de compra del cliente'. \n",
    "\n",
    "Pasa los parámetros annot, fmt, linewidths y linecolor al método heatmap() para que:\n",
    "\n",
    "los valores se muestren en las celdas del mapa de calor;\n",
    "se muestren dos puntos decimales;\n",
    "la línea que separa las celdas tenga 1 píxel de ancho;\n",
    "el color de la línea sea negro ('black').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "revenue_per_user_pivot = pd.read_csv('/datasets/revenue_pivot.csv')\n",
    "revenue_per_user_pivot = revenue_per_user_pivot.set_index('first_order_month')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(13,9)) # Establece el tamaño de la figura\n",
    "plt.title('Tamaño promedio de compra del cliente') # Da un nombre a la visualización\n",
    "# Crea un mapa de calor\n",
    "sns.heatmap(\n",
    "    revenue_per_user_pivot,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    linewidths=1,\n",
    "    linecolor='black',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASA DE RETENCIÓN Y DE CANCELACIÓN\n",
    "\n",
    "1.El DataFrame user_activity contiene datos sobre la actividad de los usuarios de una aplicación de citas para genios llamada Minder. Tiene las siguientes columnas: \n",
    "\n",
    "activity_date: todas las fechas en que el usuario escribió a sus matches por primera vez;\n",
    "user_id: el identificador del usuario.\n",
    "Crea dos columnas en el DataFrame user_activity:\n",
    "\n",
    "activity_week: la semana en que comenzó un chat;\n",
    "first_activity_week: la semana en que comenzó el primer chat.\n",
    "Para recuperar la semana, toma el primer día de la semana durante el cual ocurrió el evento. \n",
    "\n",
    "No tienes que mostrar nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lectura de la actividad del usuario desde un archivo CSV\n",
    "user_activity = pd.read_csv('/datasets/work_user_activity.csv')\n",
    "\n",
    "# Conversión de la columna 'activity_date' a formato de fecha y hora\n",
    "user_activity['activity_date'] = pd.to_datetime(user_activity['activity_date'])\n",
    "\n",
    "# Encontrar la primera fecha de actividad de cada usuario\n",
    "first_activity_date = user_activity.groupby(['user_id'])['activity_date'].min()\n",
    "first_activity_date.name = 'first_activity_date'\n",
    "\n",
    "# Unión de la primera fecha de actividad con los datos de actividad del usuario\n",
    "user_activity = user_activity.join(first_activity_date, on='user_id')\n",
    "\n",
    "user_activity['activity_week'] = pd.to_datetime(user_activity['activity_date'], unit='d') - pd.to_timedelta(user_activity['activity_date'].dt.dayofweek, unit='d')\n",
    "user_activity['first_activity_week'] = pd.to_datetime(user_activity['first_activity_date'], unit='d') - pd.to_timedelta(user_activity['first_activity_date'].dt.dayofweek, unit='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Crea la columna cohort_lifetime en el DataFrame user_activity para almacenar el número de semanas (como un número entero) entre la semana en que comenzó un chat (activity_week) y la semana en que comenzó el primer chat (first_activity_week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Lectura de la actividad del usuario desde un archivo CSV\n",
    "user_activity = pd.read_csv('/datasets/work_user_activity.csv')\n",
    "\n",
    "# Conversión de la columna 'activity_date' a formato de fecha y hora\n",
    "user_activity['activity_date'] = pd.to_datetime(user_activity['activity_date'])\n",
    "\n",
    "# Encontrar la primera fecha de actividad de cada usuario\n",
    "first_activity_date = user_activity.groupby(['user_id'])['activity_date'].min()\n",
    "first_activity_date.name = 'first_activity_date'\n",
    "\n",
    "# Unión de la primera fecha de actividad con los datos de actividad del usuario\n",
    "user_activity = user_activity.join(first_activity_date, on='user_id')\n",
    "\n",
    "# Cálculo del inicio de semana para cada fecha de actividad\n",
    "user_activity['activity_week'] = pd.to_datetime(user_activity['activity_date'], unit='d') - pd.to_timedelta(user_activity['activity_date'].dt.dayofweek, unit='d')\n",
    "\n",
    "# Cálculo del inicio de semana para la primera fecha de actividad de cada usuario\n",
    "user_activity['first_activity_week'] = pd.to_datetime(user_activity['first_activity_date'], unit='d') - pd.to_timedelta(user_activity['first_activity_date'].dt.dayofweek, unit='d')\n",
    "\n",
    "user_activity['cohort_lifetime'] = (user_activity['activity_week'] - user_activity['first_activity_week'])\n",
    "user_activity['cohort_lifetime'] = user_activity['cohort_lifetime'] / np.timedelta64(1,'W')\n",
    "user_activity['cohort_lifetime'] = user_activity['cohort_lifetime'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Soluciona este ejercicio de seis partes:\n",
    "\n",
    "Crea un DataFrame cohorts con las siguientes columnas:\n",
    "first_activity_week (con cohortes basadas en la primera actividad del usuario);\n",
    "cohort_lifetime (con semanas del ciclo de vida);\n",
    "user_id (con el número de usuarios activos).\n",
    "\n",
    "Crea el DataFrame initial_users_count donde cada cohorte tendrá el número de usuarios en la 0 semana del ciclo de vida. Deja solo estas columnas en el DataFrame:\n",
    "\n",
    "semana de cohorte: first_activity_week\n",
    "el número de usuarios activos: user_id\n",
    "\n",
    "En el DataFrame initial_users_count, cambia el nombre de la columna user_id acohort_users. Almacenará el número inicial de usuarios en una cohorte.\n",
    "\n",
    "Une los DataFrames cohorts e initial_users_count por la columna con cohortes semanales y guarda el resultado como cohorts.\n",
    "\n",
    "Calcula la tasa de retención en la columna retention.\n",
    "\n",
    "Imprime la tabla dinámica resultante retention_pivot, que reflejará el cambio en la retention por cohorte (first_activity_week) dependiendo del ciclo de vida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Lectura de la actividad del usuario desde un archivo CSV\n",
    "user_activity = pd.read_csv('/datasets/work_user_activity.csv')\n",
    "\n",
    "# Conversión de la columna 'activity_date' a formato de fecha y hora\n",
    "user_activity['activity_date'] = pd.to_datetime(user_activity['activity_date'])\n",
    "\n",
    "# Encontrar la primera fecha de actividad de cada usuario\n",
    "first_activity_date = user_activity.groupby(['user_id'])['activity_date'].min()\n",
    "first_activity_date.name = 'first_activity_date'\n",
    "\n",
    "# Unión de la primera fecha de actividad con los datos de actividad del usuario\n",
    "user_activity = user_activity.join(first_activity_date, on='user_id')\n",
    "\n",
    "# Cálculo del inicio de semana para cada fecha de actividad\n",
    "user_activity['activity_week'] = pd.to_datetime(user_activity['activity_date'], unit='d') - pd.to_timedelta(user_activity['activity_date'].dt.dayofweek, unit='d')\n",
    "\n",
    "# Cálculo del inicio de semana para la primera fecha de actividad de cada usuario\n",
    "user_activity['first_activity_week'] = pd.to_datetime(user_activity['first_activity_date'], unit='d') - pd.to_timedelta(user_activity['first_activity_date'].dt.dayofweek, unit='d')\n",
    "\n",
    "# Cálculo del ciclo de vida de la cohorte en semanas\n",
    "user_activity['cohort_lifetime'] = (user_activity['activity_week'] - user_activity['first_activity_week']) / np.timedelta64(1, 'W')\n",
    "user_activity['cohort_lifetime'] = user_activity['cohort_lifetime'].astype(int)\n",
    "\n",
    "# Creación del DataFrame de cohortes\n",
    "cohorts = user_activity.groupby(['first_activity_week', 'cohort_lifetime']).agg({'user_id': 'nunique'}).reset_index()\n",
    "\n",
    "# Cálculo del número inicial de usuarios en cada cohorte\n",
    "initial_users_count = cohorts[cohorts['cohort_lifetime'] == 0][['first_activity_week', 'user_id']]\n",
    "initial_users_count = initial_users_count.rename(columns={'user_id': 'cohort_users'})\n",
    "\n",
    "# Fusión del recuento inicial de usuarios con el DataFrame de cohortes\n",
    "cohorts = cohorts.merge(initial_users_count, on='first_activity_week')\n",
    "\n",
    "# Cálculo de la tasa de retención\n",
    "cohorts['retention'] = cohorts['user_id'] / cohorts['cohort_users']\n",
    "\n",
    "# Creación de la tabla dinámica de retención\n",
    "retention_pivot = cohorts.pivot_table(index='first_activity_week', columns='cohort_lifetime', values='retention', aggfunc='sum')\n",
    "\n",
    "# Impresión de la tabla dinámica de retención\n",
    "print(retention_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "cohort_lifetime        0         1         2         3         4         5\n",
    "first_activity_week                                                       \n",
    "2019-05-27       Resultado\n",
    "cohort_lifetime        0         1         2         3         4         5\n",
    "first_activity_week                                                       \n",
    "2019-05-27           1.0  0.507752  0.331572  0.265680  0.218464  0.022551\n",
    "2019-06-03           1.0  0.394630  0.284456  0.241532  0.116147  0.003818\n",
    "2019-06-10           1.0  0.392455  0.299044  0.219165  0.094769  0.002414\n",
    "2019-06-17           1.0  0.365544  0.248136  0.176220  0.072289  0.001758\n",
    "2019-06-24           1.0  0.326602  0.233295  0.156289  0.071133  0.002517\n",
    "2019-07-01           1.0  0.261591  0.174071  0.099262  0.009563       NaN\n",
    "2019-07-08           1.0  0.180798  0.085100  0.006546       NaN       NaN\n",
    "2019-07-15           1.0  0.115160  0.014577       NaN       NaN       NaN\n",
    "2019-07-22           1.0  0.017391       NaN       NaN       NaN       NaN\n",
    "2019-07-29           1.0       NaN       NaN       NaN       NaN       NaN    1.0  0.507752  0.331572  0.265680  0.218464  0.022551\n",
    "2019-06-03           1.0  0.394630  0.284456  0.241532  0.116147  0.003818\n",
    "2019-06-10           1.0  0.392455  0.299044  0.219165  0.094769  0.002414\n",
    "2019-06-17           1.0  0.365544  0.248136  0.176220  0.072289  0.001758\n",
    "2019-06-24           1.0  0.326602  0.233295  0.156289  0.071133  0.002517\n",
    "2019-07-01           1.0  0.261591  0.174071  0.099262  0.009563       NaN\n",
    "2019-07-08           1.0  0.180798  0.085100  0.006546       NaN       NaN\n",
    "2019-07-15           1.0  0.115160  0.014577       NaN       NaN       NaN\n",
    "2019-07-22           1.0  0.017391       NaN       NaN       NaN       NaN\n",
    "2019-07-29           1.0       NaN       NaN       NaN       NaN       NaN\n",
    "\n",
    "CALCULAR LA TASA DE CANCELACIÓN EN PYTHON\n",
    "\n",
    "1.El archivo ubicado en /datasets/churn_rate.csv contiene datos sobre cohortes para el juego Cloud Wars:\n",
    "\n",
    "users_count (el número de usuarios en una cohorte);\n",
    "lifetime (la semana del ciclo de vida de una cohorte);\n",
    "first_event_week (la semana de la cohorte).\n",
    "Exporta datos de churn_rate.csv al DataFrame cohorts. \n",
    "\n",
    "Imprime las primeras 10 filas del DataFrame cohorts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cohorts = pd.read_csv('/datasets/churn_rate.csv')\n",
    "print(cohorts.head(10))\n",
    "# Imprime las 10 primeras filas del DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "  first_event_week  lifetime  users_count\n",
    "0       2019-05-06         0           20\n",
    "1       2019-05-06         1            8\n",
    "2       2019-05-06         2            5\n",
    "3       2019-05-06         3            7\n",
    "4       2019-05-06         4            5\n",
    "5       2019-05-06         5            6\n",
    "6       2019-05-06         6            8\n",
    "7       2019-05-06         7            4\n",
    "8       2019-05-13         0           75\n",
    "9       2019-05-13         1           23\n",
    "\n",
    "2.Crea la columna churn_rate en el DataFrame cohorts para calcular la tasa de cancelación para cada cohorte.\n",
    "\n",
    "Imprime las 10 primeras filas del DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cohorts = pd.read_csv('/datasets/churn_rate.csv')\n",
    "\n",
    "cohorts['churn_rate'] = cohorts.groupby(['first_event_week'])['users_count'].pct_change()\n",
    "print(cohorts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "  first_event_week  lifetime  users_count  churn_rate\n",
    "0       2019-05-06         0           20         NaN\n",
    "1       2019-05-06         1            8   -0.600000\n",
    "2       2019-05-06         2            5   -0.375000\n",
    "3       2019-05-06         3            7    0.400000\n",
    "4       2019-05-06         4            5   -0.285714\n",
    "5       2019-05-06         5            6    0.200000\n",
    "6       2019-05-06         6            8    0.333333\n",
    "7       2019-05-06         7            4   -0.500000\n",
    "8       2019-05-13         0           75         NaN\n",
    "9       2019-05-13         1           23   -0.693333\n",
    "\n",
    "3.Compila la tabla dinámica churn_pivot para que las filas contengan cohortes, las columnas tengan semanas de ciclos de vida y los valores sean la tasa de cancelación.\n",
    "\n",
    "Muestra la tabla dinámica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cohorts = pd.read_csv('/datasets/churn_rate.csv')\n",
    "\n",
    "cohorts['churn_rate'] = cohorts.groupby(['first_event_week'])[\n",
    "    'users_count'\n",
    "].pct_change()\n",
    "\n",
    "churn_pivot = cohorts.pivot_table(\n",
    "    index='first_event_week',\n",
    "    columns='lifetime',\n",
    "    values='churn_rate',\n",
    "    aggfunc='sum',\n",
    ")\n",
    "print(churn_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "lifetime            0         1         2         3         4         5         6    7\n",
    "first_event_week                                                                      \n",
    "2019-05-06        0.0 -0.600000 -0.375000  0.400000 -0.285714  0.200000  0.333333 -0.5\n",
    "2019-05-13        0.0 -0.693333  0.217391 -0.178571  0.173913 -0.074074  0.240000  NaN\n",
    "2019-05-20        0.0 -0.658824 -0.034483  0.535714 -0.302326  0.133333       NaN  NaN\n",
    "2019-05-27        0.0 -0.426087 -0.333333  0.113636 -0.163265       NaN       NaN  NaN\n",
    "2019-06-03        0.0 -0.425000 -0.010870 -0.098901       NaN       NaN       NaN  NaN\n",
    "2019-06-10        0.0 -0.352564 -0.079208       NaN       NaN       NaN       NaN  NaN\n",
    "2019-06-17        0.0 -0.326316       NaN       NaN       NaN       NaN       NaN  NaN\n",
    "2019-06-24        0.0       NaN       NaN       NaN       NaN       NaN       NaN  NaN\n",
    "\n",
    "4.Crea un mapa de calor basado en churn_pivot que ilustre los cambios en el tamaño de compra promedio de las cohortes.\n",
    "\n",
    "Establece el tamaño de la figura: plt.figure(figsize=(13, 9)). Llama la visualización 'Churn Rate'.\n",
    "\n",
    "Haz que: \n",
    "\n",
    "Los valores se muestren en las celdas del mapa de calor.\n",
    "Los porcentajes se imprimen con un decimal.\n",
    "La línea que separa las celdas tenga 1 píxel de ancho.\n",
    "El color de la línea es negro ('black')\n",
    "\n",
    "COHORTES COMPORTAMENTALES\n",
    "\n",
    "1.El archivo ubicado en /datasets/coffee_home.csv contiene datos sobre la aplicación de una cafetería llamada CoffeeHome:\n",
    "\n",
    "coffee_time: la hora en la que pidieron el café;\n",
    "user_id: el identificador de cliente;\n",
    "first_coffee_datetime: la fecha y hora en la que se pidió café por primera vez (usado para formar cohortes - first_coffee_week)\n",
    "order_id: el identificador del pedido;\n",
    "coffee_week: la semana durante la cual se pidió el café;\n",
    "first_coffee_week: la semana durante la cual se pidió el café por primera vez (cohorte semanal);\n",
    "cohort_lifetime: el periodo del ciclo de vida.\n",
    "Los clientes de la cafetería utilizan la aplicación cada vez que piden un café. CoffeeHome tiene una oferta especial: si pides cinco tazas de café dentro de los 30 días después de tu primera compra, recibes un café gratis. Prueba la hipótesis de que los clientes que obtienen un café gratis regresan con más frecuencia y tienen tasas de retención más altas.\n",
    "\n",
    "Exporta los datos del archivo coffee_home.csv al DataFrame events. \n",
    "\n",
    "Convierte los datos en las columnas coffee_time y first_coffee_datetime al tipo datetime.\n",
    "\n",
    "Imprime las 10 primeras filas del DataFrame events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "events = pd.read_csv('/datasets/coffee_home.csv')\n",
    "events['coffee_time'] = pd.to_datetime(events['coffee_time'])\n",
    "events['first_coffee_datetime'] = pd.to_datetime(events['first_coffee_datetime'])\n",
    "print(events.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "          coffee_time user_id  ... first_coffee_week  cohort_lifetime\n",
    "0 2019-01-28 01:08:00  2d8ad2  ...        2019-01-28                0\n",
    "1 2019-01-28 11:12:12  b994e6  ...        2019-01-28                0\n",
    "2 2019-01-29 07:11:29  464d6c  ...        2019-01-28                0\n",
    "3 2019-01-29 16:42:35  59b512  ...        2019-01-28                0\n",
    "4 2019-01-30 01:15:24  d8dc9d  ...        2019-01-28                0\n",
    "5 2019-01-30 01:23:55  0e7888  ...        2019-01-28                0\n",
    "6 2019-01-30 01:50:19  36e446  ...        2019-01-28                0\n",
    "7 2019-01-30 01:56:32  6eba43  ...        2019-01-28                0\n",
    "8 2019-01-30 04:20:26  2d8ad2  ...        2019-01-28                0\n",
    "9 2019-01-30 05:22:58  ff6d8a  ...        2019-01-28                0\n",
    "\n",
    "[10 rows x 7 columns]\n",
    "\n",
    "2.En el DataFrame events crea una columna time_to_event para almacenar la diferencia entre la fecha y hora en que se ordenó un café y la fecha y hora de la primera compra.\n",
    "\n",
    "Crea un slice de datos, filtered_events, con todas las marcas temporales de menos de 30 días a partir de la fecha del primer pedido.\n",
    "\n",
    "Imprime las 10 primeras filas del segmento de datos filtered_events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "events = pd.read_csv('/datasets/coffee_home.csv')\n",
    "events['coffee_time'] = pd.to_datetime(events['coffee_time'])\n",
    "events['first_coffee_datetime'] = pd.to_datetime(\n",
    "    events['first_coffee_datetime']\n",
    ")\n",
    "\n",
    "events['time_to_event'] = events['coffee_time'] - events['first_coffee_datetime']# Calcula la diferencia entre el tiempo del pedido y el tiempo de la primera compra\n",
    "filtered_events = events[events['time_to_event'] < '30 days'] \n",
    "# Crea un slice de datos con todas las marcas temporales de menos de 30 días de la fecha del primer pedido\n",
    "# Imprime las 10 primeras filas del slice de datos\n",
    "print(filtered_events.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "          coffee_time user_id  ... cohort_lifetime   time_to_event\n",
    "0 2019-01-28 01:08:00  2d8ad2  ...               0 0 days 00:00:00\n",
    "1 2019-01-28 11:12:12  b994e6  ...               0 0 days 00:00:00\n",
    "2 2019-01-29 07:11:29  464d6c  ...               0 0 days 00:00:00\n",
    "3 2019-01-29 16:42:35  59b512  ...               0 0 days 00:00:00\n",
    "4 2019-01-30 01:15:24  d8dc9d  ...               0 0 days 00:00:00\n",
    "5 2019-01-30 01:23:55  0e7888  ...               0 0 days 00:00:00\n",
    "6 2019-01-30 01:50:19  36e446  ...               0 0 days 00:00:00\n",
    "7 2019-01-30 01:56:32  6eba43  ...               0 0 days 00:00:00\n",
    "8 2019-01-30 04:20:26  2d8ad2  ...               0 2 days 03:12:26\n",
    "9 2019-01-30 05:22:58  ff6d8a  ...               0 0 days 00:00:00\n",
    "\n",
    "[10 rows x 8 columns]\n",
    "\n",
    "3.Crea un DataFrame count_events_by_users y calcula la cantidad de registros de compra para cada cliente del slice de datos filtered_events. \n",
    "Crea la columna is_target_behavior en el DataFrame count_events_by_users. Marca a los clientes que pidieron café más de cuatro veces con True.\n",
    "Imprime las primeras 10 filas del DataFrame count_events_by_usersAgrupa a los clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "events = pd.read_csv('/datasets/coffee_home.csv')\n",
    "events['coffee_time'] = pd.to_datetime(events['coffee_time'])\n",
    "events['first_coffee_datetime'] = pd.to_datetime(\n",
    "    events['first_coffee_datetime']\n",
    ")\n",
    "\n",
    "events['time_to_event'] = (\n",
    "    events['coffee_time'] - events['first_coffee_datetime']\n",
    ")\n",
    "filtered_events = events[events['time_to_event'] < '30 days']\n",
    "\n",
    "# Group the customers\n",
    "count_events_by_users = filtered_events.groupby(['user_id']).agg({'coffee_time':'count'}).reset_index()\n",
    "# Esta columna indica que el cliente pidió café más de cuatro veces\n",
    "count_events_by_users['is_target_behavior'] = count_events_by_users['coffee_time'] > 4\n",
    "print(count_events_by_users.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "  user_id  coffee_time  is_target_behavior\n",
    "0  000cef            6                True\n",
    "1  001ab5            4               False\n",
    "2  002a14            9                True\n",
    "3  002ae6            1               False\n",
    "4  0034dc            5                True\n",
    "5  003cd3           10                True\n",
    "6  004717            1               False\n",
    "7  004889            8                True\n",
    "8  004e3b            1               False\n",
    "9  00519b            2               False\n",
    "\n",
    "4.Crea la lista user_ids_with_target_behavior para almacenar clientes con el número objetivo de pedidos (más de cuatro).\n",
    "\n",
    "Crea la lista user_ids_without_target_behavior para almacenar al resto de los clientes.\n",
    "\n",
    "En el DataFrame events, crea la columna is_in_behavioral_cohort para almacenar uno de dos valores, según el identificador del cliente (user_id):\n",
    "\n",
    "yes si el user_id está en la lista user_ids_with_target_behavior\n",
    "no si el user_id está en la lista user_ids_without_target_behavior\n",
    "Imprime las 10 primeras filas del DataFrame events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "events = pd.read_csv('/datasets/coffee_home.csv')\n",
    "events['coffee_time'] = pd.to_datetime(events['coffee_time'])\n",
    "events['first_coffee_datetime'] = pd.to_datetime(\n",
    "    events['first_coffee_datetime']\n",
    ")\n",
    "\n",
    "events['time_to_event'] = (\n",
    "    events['coffee_time'] - events['first_coffee_datetime']\n",
    ")\n",
    "filtered_events = events[events['time_to_event'] < '30 days']\n",
    "\n",
    "count_events_by_users = (\n",
    "    filtered_events.groupby(['user_id'])\n",
    "    .agg({'coffee_time': 'count'})\n",
    "    .reset_index()\n",
    ")\n",
    "count_events_by_users['is_target_behavior'] = (\n",
    "    count_events_by_users['coffee_time'] > 4\n",
    ")\n",
    "# Coloca la lista de clientes con el comportamiento objetivo (más de 4 pedidos) aquí\n",
    "user_ids_with_target_behavior = count_events_by_users[count_events_by_users['is_target_behavior']]['user_id']\n",
    "# Escribe los clientes restantes aquí\n",
    "user_ids_without_target_behavior = count_events_by_users[~count_events_by_users['is_target_behavior']]['user_id']\n",
    "#En esta parte de arriba está mal porque en realidad se tiene que usar el método query para que se pueda usar True.\n",
    "# Crea una columna dependiente de от user_id\n",
    "events.loc[events['user_id'].isin(user_ids_with_target_behavior),'is_in_behavioral_cohort'] = 'yes'\n",
    "events.loc[events['user_id'].isin(user_ids_without_target_behavior),'is_in_behavioral_cohort'] = 'no'\n",
    "events['is_in_behavioral_cohort'] = events['is_in_behavioral_cohort'].fillna('no_behavior')\n",
    "# Muestra el DataFrame events\n",
    "print(events.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "          coffee_time user_id  ...   time_to_event  is_in_behavioral_cohort\n",
    "0 2019-01-28 01:08:00  2d8ad2  ... 0 days 00:00:00                       no\n",
    "1 2019-01-28 11:12:12  b994e6  ... 0 days 00:00:00                       no\n",
    "2 2019-01-29 07:11:29  464d6c  ... 0 days 00:00:00                       no\n",
    "3 2019-01-29 16:42:35  59b512  ... 0 days 00:00:00                       no\n",
    "4 2019-01-30 01:15:24  d8dc9d  ... 0 days 00:00:00                       no\n",
    "5 2019-01-30 01:23:55  0e7888  ... 0 days 00:00:00                       no\n",
    "6 2019-01-30 01:50:19  36e446  ... 0 days 00:00:00                       no\n",
    "7 2019-01-30 01:56:32  6eba43  ... 0 days 00:00:00                       no\n",
    "8 2019-01-30 04:20:26  2d8ad2  ... 2 days 03:12:26                       no\n",
    "9 2019-01-30 05:22:58  ff6d8a  ... 0 days 00:00:00                       no\n",
    "\n",
    "[10 rows x 9 columns]\n",
    "\n",
    "5.Llama a la función printRetentionRate() para imprimir datos sobre la tasa de retención promedio en cada período del ciclo de vida para los clientes que no entraron en la cohorte por comportamiento de aquellos que pidieron café más de cuatro veces durante los primeros 30 días."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "events = pd.read_csv('/datasets/coffee_home.csv')\n",
    "events['coffee_time'] = pd.to_datetime(events['coffee_time'])\n",
    "events['first_coffee_datetime'] = pd.to_datetime(\n",
    "    events['first_coffee_datetime']\n",
    ")\n",
    "\n",
    "events['time_to_event'] = (\n",
    "    events['coffee_time'] - events['first_coffee_datetime']\n",
    ")\n",
    "filtered_events = events[events['time_to_event'] < '30 days']\n",
    "\n",
    "count_events_by_users = (\n",
    "    filtered_events.groupby(['user_id'])\n",
    "    .agg({'coffee_time': 'count'})\n",
    "    .reset_index()\n",
    ")\n",
    "count_events_by_users['is_target_behavior'] = (\n",
    "    count_events_by_users['coffee_time'] > 4\n",
    ")\n",
    "\n",
    "user_ids_with_target_behavior = count_events_by_users.query(\n",
    "    'is_target_behavior == True'\n",
    ")['user_id'].unique()\n",
    "user_ids_without_target_behavior = count_events_by_users.query(\n",
    "    'is_target_behavior != True'\n",
    ")['user_id'].unique()\n",
    "\n",
    "events.loc[\n",
    "    events['user_id'].isin(user_ids_with_target_behavior),\n",
    "    'is_in_behavioral_cohort',\n",
    "] = 'yes'\n",
    "events.loc[\n",
    "    events['user_id'].isin(user_ids_without_target_behavior),\n",
    "    'is_in_behavioral_cohort',\n",
    "] = 'no'\n",
    "\n",
    "\n",
    "def printRetentionRate(df):\n",
    "    cohorts = (\n",
    "        df.groupby(['first_coffee_week', 'cohort_lifetime'], as_index=False)\n",
    "        .agg({'user_id': 'nunique'})\n",
    "        .sort_values(['first_coffee_week', 'cohort_lifetime'])\n",
    "    )\n",
    "\n",
    "    inital_users_count = cohorts[cohorts['cohort_lifetime'] == 0][\n",
    "        ['first_coffee_week', 'user_id']\n",
    "    ]\n",
    "    inital_users_count = inital_users_count.rename(\n",
    "        columns={'user_id': 'cohort_users'}\n",
    "    )\n",
    "\n",
    "    cohorts = cohorts.merge(inital_users_count, on='first_coffee_week')\n",
    "\n",
    "    cohorts['retention'] = cohorts['user_id'] / cohorts['cohort_users']\n",
    "\n",
    "    print(cohorts.groupby(['cohort_lifetime'])['retention'].mean())\n",
    "    cohorts.groupby(['cohort_lifetime'])['retention'].mean().plot.bar()\n",
    "# Ejecuta la función aquí\n",
    "\n",
    "def printRetentionRate(df):\n",
    "    cohorts=(\n",
    "        df.groupby(['first_coffee_week','cohort_lifetime'], as_index=False)\n",
    "        .agg({'user_id':'nunique'})\n",
    "        .sort_values(['first_coffee_week','cohort_lifetime'])\n",
    "    )\n",
    "    inital_users_count = cohorts[cohorts['cohort_lifetime'] == 0][\n",
    "        ['first_coffee_week', 'user_id']\n",
    "    ]\n",
    "    inital_users_count = inital_users_count.rename(\n",
    "        columns={'user_id': 'cohort_users'}\n",
    "    )\n",
    "    cohorts = cohorts.merge(inital_users_count, on='first_coffee_week')\n",
    "    cohorts['retention'] = cohorts['user_id'] / cohorts['cohort_users']\n",
    "    print(cohorts.groupby(['cohort_lifetime'])['retention'].mean())\n",
    "    cohorts.groupby(['cohort_lifetime'])['retention'].mean().plot.bar()\n",
    "\n",
    "printRetentionRate(events[events['is_in_behavioral_cohort'] =='no'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "cohort_lifetime\n",
    "0     1.000000\n",
    "1     0.315788\n",
    "2     0.284618\n",
    "3     0.265301\n",
    "4     0.281590\n",
    "5     0.303258\n",
    "6     0.307930\n",
    "7     0.275201\n",
    "8     0.293292\n",
    "9     0.296819\n",
    "10    0.278049\n",
    "11    0.262174\n",
    "12    0.276835\n",
    "13    0.266609\n",
    "14    0.220962\n",
    "15    0.232229\n",
    "16    0.257576\n",
    "Name: retention, dtype: float64\n",
    "\n",
    "6. Llama a la función printRetentionRate() para imprimir datos sobre la tasa de retención promedio en cada período del ciclo de vida para los clientes que entraron en la cohorte por comportamiento de aquellos que pidieron café más de cuatro veces durante los primeros 30 días."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta la función aquí\n",
    "\n",
    "def printRetentionRate(df):\n",
    "    cohorts=(\n",
    "        df.groupby(['first_coffee_week','cohort_lifetime'], as_index=False)\n",
    "        .agg({'user_id':'nunique'})\n",
    "        .sort_values(['first_coffee_week','cohort_lifetime'])\n",
    "    )\n",
    "    inital_users_count = cohorts[cohorts['cohort_lifetime'] == 0][\n",
    "        ['first_coffee_week', 'user_id']\n",
    "    ]\n",
    "    inital_users_count = inital_users_count.rename(\n",
    "        columns={'user_id': 'cohort_users'}\n",
    "    )\n",
    "    cohorts = cohorts.merge(inital_users_count, on='first_coffee_week')\n",
    "    cohorts['retention'] = cohorts['user_id'] / cohorts['cohort_users']\n",
    "    print(cohorts.groupby(['cohort_lifetime'])['retention'].mean())\n",
    "    cohorts.groupby(['cohort_lifetime'])['retention'].mean().plot.bar()\n",
    "\n",
    "printRetentionRate(events[events['is_in_behavioral_cohort'] =='yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "cohort_lifetime\n",
    "0     1.000000\n",
    "1     0.762075\n",
    "2     0.713905\n",
    "3     0.751156\n",
    "4     0.570650\n",
    "5     0.417797\n",
    "6     0.403484\n",
    "7     0.355798\n",
    "8     0.308560\n",
    "9     0.341140\n",
    "10    0.462975\n",
    "11    0.344382\n",
    "12    0.328101\n",
    "13    0.253526\n",
    "14    0.276984\n",
    "15    0.475000\n",
    "16    0.500000\n",
    "Name: retention, dtype: float64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
