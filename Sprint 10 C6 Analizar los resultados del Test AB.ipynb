{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una empresa ofreció a 400 de 900 clientes potenciales (usuarios que mostraron algún interés en un producto) una oferta especial para servicios ampliados. A los 500 usuarios restantes se les ofreció el paquete de servicios original. \n",
    "En cada grupo, 100 usuarios pidieron el paquete.\n",
    "¿Funcionó la promoción? Prueba la hipótesis de que las proporciones de los pedidos en estos dos segmentos de clientes potenciales eran iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "import numpy as np\n",
    "import math as mth\n",
    "\n",
    "alpha = .05 # nivel de significación\n",
    "\n",
    "purchases = np.array([100, 100])\n",
    "leads = np.array([400, 500])\n",
    "\n",
    "# escribe tu código aquí\n",
    "p1 = purchases[0]/leads[0]\n",
    "p2 = purchases[1]/leads[1]\n",
    "p_combined = (purchases[0] + purchases[1]) / (leads[0] + leads[1])\n",
    "difference = p1 - p2\n",
    "z_value = difference / mth.sqrt(p_combined * (1 - p_combined) * (1/leads[0] + 1/leads[1]))\n",
    "\n",
    "distr = st.norm(0,1)\n",
    "\n",
    "p_value = (1 - distr.cdf(abs(z_value))) * 2 # escribe tu código aquí\n",
    "\n",
    "print('p-value: ', p_value)\n",
    "\n",
    "if (p_value < alpha): # escribe tu código aquí\n",
    "    print(\"Rechazar la hipótesis nula: hay una diferencia significativa entre las proporciones\")\n",
    "else:\n",
    "    print(\"No se pudo rechazar la hipótesis nula: no hay razón para pensar que las proporciones son diferentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value:  0.07299804543011557\n",
    "No se pudo rechazar la hipótesis nula: no hay razón para pensar que las proporciones son diferentes\n",
    "\n",
    "Pruebas de normalidad. Test de Shapiro-WilK\n",
    "\n",
    "Una muestra con recibos de la tienda online redondeados al dólar se almacena en sales_data. Comprueba si los totales de estos recibos se distribuyen normalmente utilizando el test de Shapiro-Wilk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "\n",
    "sales_data = [324,  209,  217,  321,  210,  231,  235,  519,  210,  240,  213,  325,\n",
    "  252,  251,  246,  353,  260,  256,  203,  212,  211,  318,  529,  252,\n",
    "  227,  278,  221,  222,  257,  289,  208,  256,  308,  395,  485,  350,\n",
    "  214,  378,  218,  261,  216,  289,  533,  239,  326,  445,  210,  284,\n",
    "  317,  260,  420,  497,  321,  205,  237,  261,  205,  269,  246,  685,\n",
    "  246,  207,  317,  236,  519,  230,  208,  202,  216,  234,  242,  200,\n",
    "  226,  213,  440, 1026,  318,  286,  210,  216,  227,  256,  221,  216,\n",
    "  204,  498,  223,  287,  296,  292,  406,  213,  210,  291,  217,  200,\n",
    "  344,  296,  222,  258,  223,  422,  497,  325,  328,  201,  242,  255,\n",
    "  203,  252,  254,  221,  527,  231,  506,  203,  261,  678,  209,  261,\n",
    "  281,  210,  292,  354,  210,  235,  220,  204,  270,  218,  230,  295,\n",
    "  215,  372,  218,  230,  282,  284,  229,  210,  206,  267,  299,  263,\n",
    "  563,  215,  258,  214,  351,  201]\n",
    "\n",
    "alpha = .05 # escribe tu código aquí\n",
    "\n",
    "results = st.shapiro(sales_data) # escribe tu código aquí\n",
    "p_value = results[1] # escribe tu código aquí\n",
    "\n",
    "print('p-value: ', p_value)\n",
    "\n",
    "if (p_value < alpha):# escribe tu código aquí\n",
    "    print(\"Hipótesis nula rechazada: la distribución no es normal\") # escribe tu código aquí\n",
    "else:\n",
    "    print(\"No se pudo rechazar la hipótesis nula: la distribución parece ser normal\")\n",
    "    # escribe tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "p-value:  2.3602234262062177e-16\n",
    "Hipótesis nula rechazada: la distribución no es normal\n",
    "\n",
    "Prueba No paramétrica de Wilcoxon-Mann-Whitney\n",
    "\n",
    "Tienes datos sobre el tamaño de compra promedio antes y después de que se introdujera un programa de lealtad para aquellos que realizan compras grandes: esos clientes recibieron mayor \"cashback\" de un banco famoso.  Revisa las muestras para ver si es posible decir que el programa fue un éxito.\n",
    "\n",
    "Para completar esta tarea, debes probar una hipótesis unilateral. Además de los datasets, pasarás al método otros dos parámetros. \n",
    "\n",
    "El tercer parámetro es de tipo booleano y define si debes corregir la continuidad de la distribución que se usa para describir datos discretos en la prueba. Hay que tenerlo en cuenta pero no afecta tanto el resultado. Este parámetro es True por defecto. Si intentas realizar una prueba unilateral o bilateral y configuras el cuarto parámetro para especificar la hipótesis alternativa, el parámetro booleano tiene que ser True. El cuarto parámetro se escribe como string: 'less', 'two-sided' o 'greater'. Así es como comparamos el primer dataset pasado al método con el segundo. En esta tarea, tenemos que verificar si el segundo dataset es más grande y el primero es más pequeño, por lo que estableceremos el parámetro como 'less'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as st\n",
    "\n",
    "sales_before = [25939,  14569,  15040,  28317,  21100,  13597,  62869,  46195,  13414,\n",
    "  13928,  17136,  14729,  25754,  17254,  16628,  16605,  40711,  74209,\n",
    "  14498,  32265,  13873,  16724,  22522,  14824,  21825,  32522,  14485,\n",
    "  16779,  17574,  16772,  18331,  19170,  13753,  15551,  17202,  13725,\n",
    "  15415,  16155,  49620,  33900,  23834,  25732,  16539,  24449,  14681,\n",
    "  15000,  14521,  13298,  14421,  17500,  15949,  16246,  19259,  15283,\n",
    "  14418,  18026,  25931,  14182,  13837,  23061,  14074,  25344,  19134,\n",
    "  14177,  19357,  96794,  26358,  16599,  15426,  23417,  68856,  44375,\n",
    "  14669,  39750,  34531,  14655,  28580,  25176,  55065,  64288,  16069,\n",
    "  16745,  13548,  19177,  19173,  16473,  15534,  20115,  16608,  15261,\n",
    "  13472,  47956,  21036,  19238,  25955,  14755,  16901,  13740,  13585,\n",
    "  23080,  17259,  51311,  47505,  19582,  13968,  46805,  14261,  18376,\n",
    "  13314,  37948,  18404,  16911,  18692,  19885,  16619,  15234,  21832,\n",
    " 228535,  28377,  16452,  13293,  17915,  15527,  17671,  24046,  15645,\n",
    "  14350,  16765,  17600,  14222,  25300,  16941,  14758,  17120,  14621,\n",
    "  25596,  20472,  24871,  14504,  17956,  20565,  18868,  16980,  40395,\n",
    "  13868,  14572,  13893,  17986,  14490,  16891]\n",
    "sales_after = [17484,  18369,  19412,  35496,  30841,  18511,  16438,  16064,  27841,\n",
    "  18335,  20978,  18266,  24675,  16355,  15245,  14960,  15448,  14181,\n",
    "  20095,  15586,  18594,  14414,  50452,  18804,  16750,  17313,  20047,\n",
    "  25674,  30803,  14567,  16871,  17667,  48241,  15191, 135885, 104794,\n",
    "  18650,  16708,  26201,  15926,  40253,  17787,  28374,  22989,  21122,\n",
    "  14938, 115634,  18351,  15895,  14951,  15177,  25709,  76209,  99617,\n",
    "  16452,  16446,  19407,  21144,  14947,  26257,  23723,  18113,  27784,\n",
    "  38882,  15907,  15741,  21705,  32604,  16101,  17870,  15794,  18423,\n",
    "  18381, 194987,  15335,  14022,  21257,  29935,  14598,  26066,  47228,\n",
    "  37022,  15071,  21353,  38690,  40838,  26125,  24722,  30756,  17099,\n",
    "  21377,  14611,  44442,  15808,  17173,  93187,  30411,  15279,  25707,\n",
    "  35374,  70792,  14918,  21678,  16453,  40998,  27836,  18411,  46965,\n",
    "  15968,  22812,  15856,  17933,  23682,  33450,  21727,  17884,  21676,\n",
    " 124684,  20145,  16041,  14872,  17588,  17436,  81993,  20497,  17484,\n",
    "  58826,  26179,  17515,  27463,  14260,  27331,  17598,  41888,  14037,\n",
    "  15517,  19704,  16718,  32514,  38851,  18925,  23982,  14104,  21690,\n",
    "  60266,  21071,  42799,  16203,  16694,  22699]\n",
    "\n",
    "alpha = .05# escribe tu código aquí\n",
    "\n",
    "results = st.mannwhitneyu(sales_before, sales_after, True, 'less') # escribe tu código aquí\n",
    "\n",
    "print('valor p: ', results.pvalue) # escribe tu código aquí\n",
    "\n",
    "if (results.pvalue < alpha):# escribe tu código aquí\n",
    "    print(\"Hipótesis nula rechazada: la primera serie es significativamente más pequeña que la segunda\")\n",
    "else:\n",
    "    print(\"No se rechazó la hipótesis nula: no es posible concluir que la primera serie sea menor que la segunda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "valor p:  0.00024742100920590535\n",
    "Hipótesis nula rechazada: la primera serie es significativamente más pequeña que la segunda\n",
    "\n",
    "Estabilidad de las métricas acumuladas\n",
    "\n",
    ".\n",
    "\n",
    "Estudia los datos de los archivos con los pedidos y los visitantes.\n",
    "\n",
    "La tabla de los pedidos contiene las siguientes columnas: \n",
    "\n",
    "orderId — el identificador del pedido.\n",
    "userId — el identificador del usuario que realiza el pedido.\n",
    "group — el grupo de la prueba A/B (A o B).\n",
    "revenue — los ingresos por pedidos (tamaño promedio de compra).\n",
    "date — cuando se realizó el pedido.\n",
    "La tabla de los visitantes contiene las siguientes columnas: \n",
    "\n",
    "date — fecha.\n",
    "group — grupo.\n",
    "visitors — el número de visitantes en la fecha especificada para el grupo especificado.\n",
    "1) Crea un DataFrame llamado datesGroups con parejas de valores únicos de 'date' y 'group' de la tabla orders. Deshazte de los valores duplicados con el método drop_duplicates().\n",
    "\n",
    "2) Declara la variable ordersAggregated para almacenar:\n",
    "\n",
    "la fecha;\n",
    "el grupo de la prueba A/B;\n",
    "el número de pedidos distintos para el grupo de prueba hasta la fecha especificada incluida;\n",
    "el número de usuarios distintos en el grupo de prueba que realizan al menos un pedido hasta la fecha especificada incluida;\n",
    "los ingresos totales de pedidos en el grupo de prueba hasta la fecha especificada incluida.\n",
    "3) Declara la variable visitorsAggregated para almacenar:\n",
    "\n",
    "la fecha;\n",
    "el grupo de la prueba A/B;\n",
    "el número de pedidos distintos para el grupo de prueba hasta la fecha especificada incluida.\n",
    "4) Ordena ordersAggregated y visitorsAggregated por las columnas 'date' y 'group', en ese orden. \n",
    "\n",
    "5) Define la variable cumulativeData uniendo ordersAggregated y visitorsAggregated por las columnas 'date' y 'group' con el método merge().\n",
    "\n",
    "6) Asigna los nombres ['date', 'group', 'orders', 'buyers', 'revenue', 'visitors'] a las columnas en cumulativeData.\n",
    "\n",
    "7) Imprime las primeras cinco filas de cumulativeData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "orders = pd.read_csv('/datasets/data_for_tasks_3.csv', sep=',')\n",
    "orders['date'] = orders['date'].map(\n",
    "    lambda x: dt.datetime.strptime(x, '%d/%m/%Y')\n",
    ")\n",
    "\n",
    "visitors = pd.read_csv('/datasets/data_for_tasks_3_visitors.csv', sep=',')\n",
    "visitors['date'] = visitors['date'].map(\n",
    "    lambda x: dt.datetime.strptime(x, '%d/%m/%Y')\n",
    ")\n",
    "\n",
    "print(orders.head(5))\n",
    "print(visitors.head(5))\n",
    "\n",
    "datesGroups = orders[['date', 'group']].drop_duplicates()\n",
    "\n",
    "ordersAggregated = datesGroups.apply(\n",
    "    lambda x: orders[\n",
    "        np.logical_and(\n",
    "            orders['date'] <= x['date'], orders['group'] == x['group']\n",
    "        )\n",
    "    ].agg(\n",
    "        {\n",
    "            'date': 'max',\n",
    "            'group': 'max',\n",
    "            'orderId': pd.Series.nunique,\n",
    "            'userId': pd.Series.nunique,\n",
    "            'revenue': 'sum',\n",
    "        }\n",
    "    ),\n",
    "    axis=1,\n",
    ").sort_values(by=['date', 'group'])\n",
    "\n",
    "visitorsAggregated = datesGroups.apply(\n",
    "    lambda x: visitors[\n",
    "        np.logical_and(\n",
    "            visitors['date'] <= x['date'], visitors['group'] == x['group']\n",
    "        )\n",
    "    ].agg({'date': 'max', 'group': 'max', 'visitors': 'sum'}),\n",
    "    axis=1,\n",
    ").sort_values(by=['date', 'group'])\n",
    "\n",
    "cumulativeData = ordersAggregated.merge(\n",
    "    visitorsAggregated, left_on=['date', 'group'], right_on=['date', 'group']\n",
    ")\n",
    "cumulativeData.columns = [\n",
    "    'date',\n",
    "    'group',\n",
    "    'orders',\n",
    "    'buyers',\n",
    "    'revenue',\n",
    "    'visitors',\n",
    "]\n",
    "\n",
    "print(cumulativeData.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "         userId group       orderId  revenue       date\n",
    "0  1.815263e+19     B  4.612878e+15      489 2019-04-22\n",
    "1  1.815263e+19     B  4.612878e+15      489 2019-04-22\n",
    "2  1.794078e+19     B  4.136278e+18       97 2019-04-22\n",
    "3  1.794078e+19     B  4.136278e+18      279 2019-04-22\n",
    "4  2.461477e+18     B  1.406554e+19     4092 2019-04-22\n",
    "        date group  visitors\n",
    "0 2019-03-11     A       321\n",
    "1 2019-03-12     A       831\n",
    "2 2019-03-13     A       700\n",
    "3 2019-03-14     A      1222\n",
    "4 2019-03-15     A       859\n",
    "        date group  orders  buyers  revenue  visitors\n",
    "0 2019-03-11     A      10      10   110291       321\n",
    "1 2019-03-11     B       9       9    36646       337\n",
    "2 2019-03-12     A      35      33   343089      1152\n",
    "3 2019-03-12     B      35      33    90491      1198\n",
    "4 2019-03-13     A      59      56  1012663      1852\n",
    "\n",
    "2.Declara las variables cumulativeRevenueA y cumulativeRevenueB, donde almacenarás los datos sobre fechas, ingresos y número de pedidos para los grupos A y B.\n",
    "Traza gráficos de ingresos acumulados diarios para cada grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "orders = pd.read_csv('/datasets/data_for_tasks_3.csv', sep=',')\n",
    "orders['date'] = orders['date'].map(\n",
    "    lambda x: dt.datetime.strptime(x, '%d/%m/%Y')\n",
    ")\n",
    "\n",
    "visitors = pd.read_csv('/datasets/data_for_tasks_3_visitors.csv', sep=',')\n",
    "visitors['date'] = visitors['date'].map(\n",
    "    lambda x: dt.datetime.strptime(x, '%d/%m/%Y')\n",
    ")\n",
    "\n",
    "datesGroups = orders[['date', 'group']].drop_duplicates()\n",
    "\n",
    "ordersAggregated = datesGroups.apply(\n",
    "    lambda x: orders[\n",
    "        np.logical_and(\n",
    "            orders['date'] <= x['date'], orders['group'] == x['group']\n",
    "        )\n",
    "    ].agg(\n",
    "        {\n",
    "            'date': 'max',\n",
    "            'group': 'max',\n",
    "            'orderId': pd.Series.nunique,\n",
    "            'userId': pd.Series.nunique,\n",
    "            'revenue': 'sum',\n",
    "        }\n",
    "    ),\n",
    "    axis=1,\n",
    ").sort_values(by=['date', 'group'])\n",
    "\n",
    "visitorsAggregated = datesGroups.apply(\n",
    "    lambda x: visitors[\n",
    "        np.logical_and(\n",
    "            visitors['date'] <= x['date'], visitors['group'] == x['group']\n",
    "        )\n",
    "    ].agg({'date': 'max', 'group': 'max', 'visitors': 'sum'}),\n",
    "    axis=1,\n",
    ").sort_values(by=['date', 'group'])\n",
    "\n",
    "cumulativeData = ordersAggregated.merge(\n",
    "    visitorsAggregated, left_on=['date', 'group'], right_on=['date', 'group']\n",
    ")\n",
    "cumulativeData.columns = [\n",
    "    'date',\n",
    "    'group',\n",
    "    'orders',\n",
    "    'buyers',\n",
    "    'revenue',\n",
    "    'visitors',\n",
    "]\n",
    "\n",
    "cumulativeRevenueA = cumulativeData[cumulativeData['group']=='A'][['date','revenue', 'orders']]\n",
    "cumulativeRevenueB = cumulativeData[cumulativeData['group']=='B'][['date','revenue', 'orders']]\n",
    "plt.plot(cumulativeRevenueA['date'], cumulativeRevenueA['revenue'], label='A')\n",
    "plt.plot(cumulativeRevenueB['date'], cumulativeRevenueB['revenue'], label='B')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Para cada grupo, traza gráficos acumulados (por día) para el tamaño promedio de compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulativeRevenueA['date'], cumulativeRevenueA['revenue']/cumulativeRevenueA['orders'], label='A')\n",
    "plt.plot(cumulativeRevenueB['date'], cumulativeRevenueB['revenue']/cumulativeRevenueB['orders'], label='B')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Une las tablas cumulativeRevenueA y cumulativeRevenueB con el método merge() para que la tabla resultante contenga las columnas ['date', 'revenueA', 'revenueB', 'ordersA', 'ordersB']. Guárdala en mergedCumulativeRevenue. \n",
    "\n",
    "Muestra con un gráfico la diferencia relativa entre el tamaño de compra promedio acumulado del grupo B en comparación con el del grupo A. \n",
    "\n",
    "Agrega un eje X de línea punteada (Y = 0) con el método plt.axhline()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reunir los datos en un DataFrame\n",
    "mergedCumulativeRevenue = cumulativeRevenueA.merge(cumulativeRevenueB, left_on='date', right_on='date', how='left', suffixes=['A', 'B'])\n",
    "\n",
    "# trazar un gráfico de diferencia relativa para los tamaños de compra promedio\n",
    "plt.plot(mergedCumulativeRevenue['date'], (mergedCumulativeRevenue['revenueB']/mergedCumulativeRevenue['ordersB'])/(mergedCumulativeRevenue['revenueA']/mergedCumulativeRevenue['ordersA'])-1)\n",
    "\n",
    "# agregar el eje X\n",
    "plt.axhline(y=0, color='black', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Agrega la columna 'conversion' a cumulativeData.  Esta columna contiene la relación entre el número de pedidos y el número de usuarios para un grupo específico en una fecha específica. \n",
    "\n",
    "Declara las variables cumulativeDataA y cumulativeDataB, donde almacenarás los datos de los pedidos en los segmentos A y B. \n",
    "\n",
    "Traza el gráfico de conversión acumulada diaria de cada grupo. \n",
    "\n",
    "Configura la escala de los ejes de la siguiente manera: plt.axis([pd.to_datetime(\"2019-03-10\"), pd.to_datetime('2019-04-23'), 0, 0.05])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionar datos en el grupo A\n",
    "cumulativeDataA = cumulativeData[cumulativeData['group']=='A']\n",
    "\n",
    "# seleccionar datos en el grupo B\n",
    "cumulativeDataB = cumulativeData[cumulativeData['group']=='B']\n",
    "\n",
    "# trazar los gráficos\n",
    "plt.plot(cumulativeDataA['date'], cumulativeDataA['conversion'], label='A')\n",
    "plt.plot(cumulativeDataB['date'], cumulativeDataB['conversion'], label='B')\n",
    "plt.legend()\n",
    "\n",
    "# establecer la escala de los ejes\n",
    "plt.axis([pd.to_datetime(\"2019-03-10\"),pd.to_datetime('2019-04-23'), 0, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Une las tablas cumulativeDataA y cumulativeDataB utilizando el método merge(), de modo que la tabla resultante contenga las columnas ['date', 'conversionA', 'conversionB']. Guárdala en mergedCumulativeConversions.\n",
    "\n",
    "Traza la diferencia relativa entre la tasa de conversión acumulada del grupo B en comparación con la del grupo A. \n",
    "\n",
    "Con el método plt.axhline(), agrega una línea punteada del eje X (Y = 0) y establece su color como 'black'. Agrega un eje X más en Y = 0.2 de color 'grey'.\n",
    "\n",
    "Establece la escala de los ejes de la siguiente manera: plt.axis([pd.to_datetime(\"2019-03-10\"), pd.to_datetime('2019-04-23'), -0.5, 0.5])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulativeDataA = cumulativeData[cumulativeData['group'] == 'A']\n",
    "cumulativeDataB = cumulativeData[cumulativeData['group'] == 'B']\n",
    "\n",
    "mergedCumulativeConversions = cumulativeDataA[['date','conversion']].merge(cumulativeDataB[['date','conversion']], left_on='date', right_on='date', how='left', suffixes=['A', 'B'])\n",
    "\n",
    "plt.plot(mergedCumulativeConversions['date'], mergedCumulativeConversions['conversionB']/mergedCumulativeConversions['conversionA']-1)\n",
    "\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.axhline(y=0.2, color='grey', linestyle='--')\n",
    "plt.axis([pd.to_datetime(\"2019-03-10\"), pd.to_datetime('2019-04-23'), -0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizar valores atípicos y aumentos: valores extremos\n",
    "\n",
    "\n",
    "1.El archivo data_for_tasks_3.csv contiene datos sobre los pedidos que los usuarios realizaron en una tienda online. Las columnas son:\n",
    "\n",
    "orderId\n",
    "userId\n",
    "group\n",
    "revenue\n",
    "date\n",
    "Encuentra el número de pedidos por usuario. Para hacerlo, crea un DataFrame con dos columnas: 'userId' y 'orders'. Llámalo ordersByUsers. Ordena los datos por el número de pedidos en orden descendente e imprime las primeras 10 filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "data = pd.read_csv('/datasets/data_for_tasks_3.csv', sep=',')\n",
    "data['date'] = data['date'].map(lambda x: dt.datetime.strptime(x, '%d/%m/%Y'))\n",
    "\n",
    "print(data.head(5))\n",
    "# El método .groupby('...', as_index=False) agrupará los pedidos por usuarios\n",
    "# El método .agg({'...' : '...',}) calculará las métricas agregadas en los datos agrupados\n",
    "# El método pd.Series.nunique encontrará el número de pedidos distintos\n",
    "# El método .sort_values(by='...',ascending=False) ordenará el DataFrame por el valor de la columna\n",
    "ordersByUsers = (data.groupby('userID', as_index=False).agg({'orderId':pd.Series.nunique}))\n",
    "ordersByUsers.columns = ['userId','orders']\n",
    "\n",
    "print(ordersByUsers.sort_values(by='orders', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "         userId group       orderId  revenue       date\n",
    "0  1.815263e+19     B  4.612878e+15      489 2019-04-22\n",
    "1  1.815263e+19     B  4.612878e+15      489 2019-04-22\n",
    "2  1.794078e+19     B  4.136278e+18       97 2019-04-22\n",
    "3  1.794078e+19     B  4.136278e+18      279 2019-04-22\n",
    "4  2.461477e+18     B  1.406554e+19     4092 2019-04-22\n",
    "            userId  orders\n",
    "1570  1.724475e+19    13.0\n",
    "1085  1.172579e+19    10.0\n",
    "122   1.216361e+18    10.0\n",
    "567   6.234396e+18     7.0\n",
    "1227  1.323237e+19     6.0\n",
    "48    4.867316e+17     6.0\n",
    "1681  1.823218e+19     6.0\n",
    "437   4.843340e+18     6.0\n",
    "337   3.862204e+18     5.0\n",
    "1422  1.549293e+19     5.0\n",
    "\n",
    "2.Traza un histograma de distribución para el número de pedidos por usuario con el método hist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ordersByUsers['orders'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "            userId  orders\n",
    "1570  1.724475e+19    13.0\n",
    "1085  1.172579e+19    10.0\n",
    "122   1.216361e+18    10.0\n",
    "567   6.234396e+18     7.0\n",
    "1227  1.323237e+19     6.0\n",
    "48    4.867316e+17     6.0\n",
    "1681  1.823218e+19     6.0\n",
    "437   4.843340e+18     6.0\n",
    "337   3.862204e+18     5.0\n",
    "1422  1.549293e+19     5.0\n",
    "\n",
    "\n",
    "3.Crea un diagrama de dispersión con el método scatter(). No olvides pasarle valores para los ejes X e Y.\n",
    "\n",
    "Puedes encontrar los valores para el eje horizontal en el precódigo en x_values: los números generados de observaciones. Toma los valores del eje vertical de la columna 'orders' del DataFrame ordersByUsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#el rango de números desde 0 hasta el número de observaciones en ordersByUsers\n",
    "x_values = pd.Series(range(0,len(ordersByUsers)))\n",
    "plt.scatter(x_values, ordersByUsers['orders'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "            userId  orders\n",
    "1570  1.724475e+19    13.0\n",
    "1085  1.172579e+19    10.0\n",
    "122   1.216361e+18    10.0\n",
    "567   6.234396e+18     7.0\n",
    "1227  1.323237e+19     6.0\n",
    "48    4.867316e+17     6.0\n",
    "1681  1.823218e+19     6.0\n",
    "437   4.843340e+18     6.0\n",
    "337   3.862204e+18     5.0\n",
    "1422  1.549293e+19     5.0\n",
    "\n",
    "4.Calcula los percentiles 90, 95 y 99 de la muestra para el número de pedidos por usuario con el método np.percentile(). Muestra los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile(ordersByUsers['orders'], [90,95,99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "            userId  orders\n",
    "1570  1.724475e+19    13.0\n",
    "1085  1.172579e+19    10.0\n",
    "122   1.216361e+18    10.0\n",
    "567   6.234396e+18     7.0\n",
    "1227  1.323237e+19     6.0\n",
    "48    4.867316e+17     6.0\n",
    "1681  1.823218e+19     6.0\n",
    "437   4.843340e+18     6.0\n",
    "337   3.862204e+18     5.0\n",
    "1422  1.549293e+19     5.0\n",
    "[1.   2.   3.02]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5.Traza un histograma de distribución de los ingresos de los pedidos ('revenue') utilizando el método hist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['revenue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "            userId group       orderId  revenue       date\n",
    "2401  1.077474e+19     B  1.410074e+19  1955100 2019-04-05\n",
    "2400  1.077474e+19     B  1.410074e+19  1955100 2019-04-05\n",
    "4775  1.724475e+19     A  1.216495e+19   947700 2019-03-18\n",
    "405   5.829727e+18     A  7.037920e+18   579081 2019-04-18\n",
    "5334  9.419044e+18     B  9.456631e+18   570000 2019-03-14\n",
    "5333  9.419044e+18     B  9.456631e+18   570000 2019-03-14\n",
    "5411  1.724475e+19     A  4.155511e+17   522900 2019-03-13\n",
    "278   1.279059e+19     A  6.765099e+18   490000 2019-04-19\n",
    "277   1.279059e+19     A  6.765099e+18   490000 2019-04-19\n",
    "4218  7.095291e+18     A  5.752271e+18   449100 2019-03-23\n",
    "\n",
    "6.Crea un gráfico de dispersión utilizando el método scatter(). No olvides pasarle valores para los ejes X e Y.\n",
    "\n",
    "Puedes encontrar los valores para el eje horizontal en el precódigo en x_values: los números generados de observaciones. Toma los valores del eje vertical de la columna 'revenue' del DataFrame data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.sort_values(by='revenue',ascending=False).head(10))\n",
    "\n",
    "x_values = pd.Series(range(0,len(data['revenue'])))\n",
    "plt.scatter(x_values, data['revenue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "            userId group       orderId  revenue       date\n",
    "2401  1.077474e+19     B  1.410074e+19  1955100 2019-04-05\n",
    "2400  1.077474e+19     B  1.410074e+19  1955100 2019-04-05\n",
    "4775  1.724475e+19     A  1.216495e+19   947700 2019-03-18\n",
    "405   5.829727e+18     A  7.037920e+18   579081 2019-04-18\n",
    "5334  9.419044e+18     B  9.456631e+18   570000 2019-03-14\n",
    "5333  9.419044e+18     B  9.456631e+18   570000 2019-03-14\n",
    "5411  1.724475e+19     A  4.155511e+17   522900 2019-03-13\n",
    "278   1.279059e+19     A  6.765099e+18   490000 2019-04-19\n",
    "277   1.279059e+19     A  6.765099e+18   490000 2019-04-19\n",
    "4218  7.095291e+18     A  5.752271e+18   449100 2019-03-23\n",
    "\n",
    "\n",
    "7.Calcula los percentiles 90, 95 y 99 de la muestra para los ingresos de pedidos por usuario con el método np.percentile()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head(10))\n",
    "\n",
    "print('Percentiles 90/95/99:',np.percentile(data['revenue'],[90,95,99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "         userId group       orderId  revenue       date\n",
    "0  1.815263e+19     B  4.612878e+15      489 2019-04-22\n",
    "1  1.815263e+19     B  4.612878e+15      489 2019-04-22\n",
    "2  1.794078e+19     B  4.136278e+18       97 2019-04-22\n",
    "3  1.794078e+19     B  4.136278e+18      279 2019-04-22\n",
    "4  2.461477e+18     B  1.406554e+19     4092 2019-04-22\n",
    "5  1.419853e+19     A  5.370143e+18      139 2019-04-22\n",
    "6  1.419853e+19     A  5.370143e+18      370 2019-04-22\n",
    "7  1.419853e+19     A  5.370143e+18     1732 2019-04-22\n",
    "8  1.419853e+19     A  5.370143e+18      174 2019-04-22\n",
    "9  1.419853e+19     A  5.370143e+18      399 2019-04-22\n",
    "Percentiles 90/95/99: [ 3899.   7740.  43569.9]\n",
    "\n",
    "Análisis paso a paso de una prueba A/B\n",
    "\n",
    "1.Estudia la estructura de la tabla data. Tomando el código de muestra de la lección como ejemplo, crea las variables ordersByUsersA y ordersByUsersB con las columnas ['userId', 'orders'], donde el número de pedidos para los usuarios con al menos un pedido será especificado. \n",
    "\n",
    "Declara las variables sampleA y sampleB con los usuarios que realizaron pedidos y los números de pedidos correspondientes. Los usuarios sin pedidos tendrán un 0. Utiliza el código de muestra de la lección como referencia. \n",
    "\n",
    "Calcula la significancia estadística de la diferencia en la conversión basada en los resultados después de dos semanas de prueba. Aplica la prueba de Mann-Whitney.  \n",
    "\n",
    "Imprime el valor p para comparar la conversión de los grupos. Redondea a cinco decimales. \n",
    "\n",
    "Calcula e imprime la diferencia relativa en la conversión entre los grupos. Redondea el resultado a tres decimales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
