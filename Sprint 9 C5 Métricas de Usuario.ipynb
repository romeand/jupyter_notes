{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.El archivo /datasets/users_data.csv almacena datos sobre la actividad del usuario en la aplicación. Léelo y guárdalo en la variable users_data. Calcula la métrica de MAU para todo el período y guarda el resultado como mau_total. Imprímelo como un solo número entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users_data = pd.read_csv('/datasets/users_data.csv')\n",
    "users_data['session_start_ts'] = pd.to_datetime(\n",
    "    users_data['session_start_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_end_ts'] = pd.to_datetime(\n",
    "    users_data['session_end_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_year'] = users_data['session_start_ts'].dt.isocalendar().year\n",
    "users_data['session_month'] = users_data['session_start_ts'].dt.month\n",
    "users_data['session_week'] = users_data['session_start_ts'].dt.isocalendar().week\n",
    "users_data['session_date'] = users_data['session_start_ts'].dt.date\n",
    "mau_total = (\n",
    "    users_data.groupby(['session_year', 'session_month'])\n",
    "    .agg({'id': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "print(int(mau_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "25429\n",
    "\n",
    "2. Echa otro vistazo a mau_total en el precódigo y encuentra dau_total y wau_total usando el mismo enfoque. \n",
    "\n",
    "Encuentra el sticky factor expresado como la relación entre la audiencia mensual y semanal. Guarda el resultado para la audiencia semanal como sticky_wau y el de la audiencia mensual como sticky_mau. Multiplica el resultado de tu división de sticky factor por 100 para obtener un porcentaje. Imprime los resultados (primero por semana, luego por mes) sin convertirlos a números enteros.\n",
    "\n",
    "Al agregar columnas a tus datos originales, pasa de mayor a menor: año, mes, semana, fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users_data = pd.read_csv('/datasets/users_data.csv')\n",
    "users_data['session_start_ts'] = pd.to_datetime(\n",
    "    users_data['session_start_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_end_ts'] = pd.to_datetime(\n",
    "    users_data['session_end_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_year'] = users_data['session_start_ts'].dt.isocalendar().year\n",
    "users_data['session_month'] = users_data['session_start_ts'].dt.month\n",
    "users_data['session_week'] = users_data['session_start_ts'].dt.isocalendar().week\n",
    "users_data['session_date'] = users_data['session_start_ts'].dt.date\n",
    "mau_total = (\n",
    "    users_data.groupby(['session_year', 'session_month'])\n",
    "    .agg({'id': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "dau_total = users_data.groupby('session_date').agg({'id': 'nunique'}).mean()\n",
    "wau_total = (\n",
    "    users_data.groupby(['session_year', 'session_week'])\n",
    "    .agg({'id': 'nunique'})\n",
    "    .mean()\n",
    ")\n",
    "sticky_wau = dau_total / wau_total * 100\n",
    "print(sticky_wau)\n",
    "sticky_mau = dau_total / mau_total * 100\n",
    "print(sticky_mau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "id    18.512582\n",
    "dtype: float64\n",
    "id    6.040741\n",
    "dtype: float64\n",
    "\n",
    "SESIONES DE USUARIO\n",
    "\n",
    "1.Sigue trabajando con la aplicación de entrega por drones. Encuentra el número de sesiones por usuario para cada mes, guardando el resultado como sessions_per_user['sess_per_user']. Imprime sessions_per_user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users_data = pd.read_csv('/datasets/users_data.csv')\n",
    "users_data['session_start_ts'] = pd.to_datetime(\n",
    "    users_data['session_start_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_end_ts'] = pd.to_datetime(\n",
    "    users_data['session_end_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_year'] = users_data['session_start_ts'].dt.year\n",
    "users_data['session_month'] = users_data['session_start_ts'].dt.month\n",
    "\n",
    "sessions_per_user = users_data.groupby(['session_year','session_month']).agg({'id':['count','nunique']})\n",
    "sessions_per_user.columns = ['n_sessions','n_users']\n",
    "sessions_per_user['sess_per_user'] = (sessions_per_user['n_sessions'] / sessions_per_user['n_users'])\n",
    "print(sessions_per_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "                            n_sessions  n_users  sess_per_user\n",
    "session_year session_month\n",
    "2018         5                   36716    26975       1.361112\n",
    "             6                   47607    33473       1.422251\n",
    "             7                   21586    15839       1.362839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Encuentra la duración de las sesiones y traza un histograma con 100 contenedores. Asigna a la columna el nombre users_data['session_duration_sec']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users_data = pd.read_csv('/datasets/users_data.csv')\n",
    "users_data['session_start_ts'] = pd.to_datetime(\n",
    "    users_data['session_start_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_end_ts'] = pd.to_datetime(\n",
    "    users_data['session_end_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_duration_sec'] = (users_data['session_end_ts'] - users_data['session_start_ts']).dt.seconds\n",
    "\n",
    "users_data['session_duration_sec'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Encuentra la métrica ASL (duración media de la sesión). Guarda el resultado como asl e imprime la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users_data = pd.read_csv('/datasets/users_data.csv')\n",
    "users_data['session_start_ts'] = pd.to_datetime(\n",
    "    users_data['session_start_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_end_ts'] = pd.to_datetime(\n",
    "    users_data['session_end_ts'], format=\"%Y-%m-%d %H:%M\"\n",
    ")\n",
    "users_data['session_duration_sec'] = (\n",
    "    users_data['session_end_ts'] - users_data['session_start_ts']\n",
    ").dt.seconds\n",
    "users_data['session_duration_sec'].hist(bins=100)\n",
    "\n",
    "asl = users_data['session_duration_sec'].mode()\n",
    "print(asl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0    60\n",
    "dtype: int64\n",
    "\n",
    "TRABAJAR CON DATOS SIN PROCESAR\n",
    "\n",
    "1.Carga los datos del archivo /datasets/analytics_logs.csv al DataFrame logs. Imprime las primeras 10 filas de logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs = pd.read_csv('/datasets/analytics_logs.csv') # Cargar datos\n",
    "print(logs.head(10)) # imprime las 10 primeras filas del DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "   ym:s:visitID     ym:s:dateTime  ...  ym:s:clientID  ym:s:lastTrafficSource\n",
    "0  2.613060e+18  15/05/2019 11:53  ...   1.557910e+18                      ad\n",
    "1  2.579540e+18  14/05/2019 00:22  ...   1.539000e+18                 organic\n",
    "2  2.788060e+18  23/05/2019 05:19  ...   1.558580e+18                 organic\n",
    "3  2.456460e+18  08/05/2019 13:57  ...   1.557310e+18                      ad\n",
    "4  2.069900e+18  21/04/2019 12:20  ...   1.555840e+19                  direct\n",
    "5  1.641560e+18  02/04/2019 14:27  ...   1.554200e+19                  direct\n",
    "6  1.678280e+18  04/04/2019 05:22  ...   1.554340e+18                 organic\n",
    "7  2.912900e+18  28/05/2019 17:37  ...   1.559050e+18                 organic\n",
    "8  1.958510e+18  16/04/2019 14:18  ...   1.555410e+17                 organic\n",
    "9  2.634970e+18  16/05/2019 11:06  ...   1.557990e+18                      ad\n",
    "\n",
    "[10 rows x 6 columns]\n",
    "\n",
    "\n",
    "2.Crea un slice de datos logs_with_duration para almacenar todas las visitas de más de 0 segundos.\n",
    "\n",
    "Imprime las primeras 10 filas del slice de datos logs_with_duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs = pd.read_csv('/datasets/analytics_logs.csv')\n",
    "\n",
    "logs_with_duration = logs[logs['ym:s:visitDuration'] > 0] # Crea un slice de datos\n",
    "\n",
    "print(logs_with_duration.head(10))\n",
    "# Imprime las 10 primeras filas del slice de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "    ym:s:visitID     ym:s:dateTime  ...  ym:s:clientID  ym:s:lastTrafficSource\n",
    "0   2.613060e+18  15/05/2019 11:53  ...   1.557910e+18                      ad\n",
    "1   2.579540e+18  14/05/2019 00:22  ...   1.539000e+18                 organic\n",
    "3   2.456460e+18  08/05/2019 13:57  ...   1.557310e+18                      ad\n",
    "4   2.069900e+18  21/04/2019 12:20  ...   1.555840e+19                  direct\n",
    "5   1.641560e+18  02/04/2019 14:27  ...   1.554200e+19                  direct\n",
    "7   2.912900e+18  28/05/2019 17:37  ...   1.559050e+18                 organic\n",
    "8   1.958510e+18  16/04/2019 14:18  ...   1.555410e+17                 organic\n",
    "10  2.274250e+18  30/04/2019 12:52  ...   1.556620e+19                referral\n",
    "11  1.979300e+18  17/04/2019 12:20  ...   1.555490e+18                  direct\n",
    "12  1.610030e+18  01/04/2019 05:02  ...   1.554080e+18                internal\n",
    "\n",
    "[10 rows x 6 columns]\n",
    "\n",
    "3.Toma el DataFrame logs_with_duration y crea dos slices de datos:\n",
    "Un slice de datos ad_visits que contiene visitas que provienen del tráfico de anuncios (ym:s:lastTrafficSource == 'ad');\n",
    "Un slice de datos organic_visits que contiene visitas \"orgánicas\" que provienen del tráfico del motor de búsqueda (ym:s:lastTrafficSource == 'organic').\n",
    "El nombre de la fuente de tráfico se almacena en la columna \"ym:s:lastTrafficSource\".\n",
    "Imprime 10 filas de cada slice de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs = pd.read_csv('/datasets/analytics_logs.csv')\n",
    "\n",
    "logs_with_duration = logs[logs['ym:s:visitDuration'] > 0]\n",
    "\n",
    "ad_visits = logs_with_duration[logs_with_duration['ym:s:lastTrafficSource'] == 'ad'] \n",
    "# Haz un slice de datos para visitas del tráfico de anuncios\n",
    "organic_visits = logs_with_duration[logs_with_duration['ym:s:lastTrafficSource'] == 'organic'] \n",
    "# Haz un slice de datos para visitas del tráfico del motor de búsqueda\n",
    "\n",
    "# Imprime 10 filas del slice de datos de tráfico de anuncios\n",
    "print(ad_visits.head(10))\n",
    "print(organic_visits.head(10))\n",
    "# Imprime 10 filas del slice de datos de tráfico del motor de búsqueda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultado\n",
    "    ym:s:visitID     ym:s:dateTime  ...  ym:s:clientID  ym:s:lastTrafficSource\n",
    "0   2.613060e+18  15/05/2019 11:53  ...   1.557910e+18                      ad\n",
    "3   2.456460e+18  08/05/2019 13:57  ...   1.557310e+18                      ad\n",
    "14  2.269310e+18  30/04/2019 07:39  ...   1.540320e+18                      ad\n",
    "15  2.167530e+18  25/04/2019 19:48  ...   1.556210e+18                      ad\n",
    "19  2.913260e+18  28/05/2019 18:00  ...   1.559060e+18                      ad\n",
    "21  2.928350e+18  29/05/2019 09:59  ...   1.559110e+18                      ad\n",
    "34  2.120520e+18  23/04/2019 17:59  ...   1.556020e+18                      ad\n",
    "37  2.013170e+18  19/04/2019 00:13  ...   1.555620e+18                      ad\n",
    "38  2.316790e+18  02/05/2019 09:57  ...   1.556780e+18                      ad\n",
    "40  2.120120e+18  23/04/2019 17:33  ...   1.555760e+18                      ad\n",
    "\n",
    "[10 rows x 6 columns]\n",
    "    ym:s:visitID     ym:s:dateTime  ...  ym:s:clientID  ym:s:lastTrafficSource\n",
    "1   2.579540e+18  14/05/2019 00:22  ...   1.539000e+18                 organic\n",
    "7   2.912900e+18  28/05/2019 17:37  ...   1.559050e+18                 organic\n",
    "8   1.958510e+18  16/04/2019 14:18  ...   1.555410e+17                 organic\n",
    "20  2.324900e+18  02/05/2019 18:33  ...   1.556810e+19                 organic\n",
    "22  2.721910e+18  20/05/2019 07:14  ...   1.558330e+18                 organic\n",
    "24  2.881280e+18  27/05/2019 08:06  ...   1.558930e+18                 organic\n",
    "28  2.777200e+18  22/05/2019 17:49  ...   1.558540e+18                 organic\n",
    "32  2.781670e+18  22/05/2019 22:33  ...   1.558550e+18                 organic\n",
    "39  1.692510e+18  04/04/2019 20:26  ...   1.554400e+18                 organic\n",
    "46  2.594680e+18  14/05/2019 16:25  ...   1.557840e+18                 organic\n",
    "\n",
    "[10 rows x 6 columns]\n",
    "\n",
    "4.Utiliza una visualización para mostrar dos histogramas:\n",
    "\n",
    "La distribución de la duración de la visita para el tráfico de anuncios (ym:s:lastTrafficSource = 'ad').\n",
    "La distribución de la duración de la visita para el tráfico del motor de búsqueda (ym:s:lastTrafficSource = 'organic').\n",
    "Los histogramas deben incluir datos dentro del rango de 0 a 2000 segundos.\n",
    "\n",
    "Para dar cierta transparencia a los histogramas, especifica alpha=0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logs = pd.read_csv('/datasets/analytics_logs.csv')\n",
    "\n",
    "logs_with_duration = logs[logs['ym:s:visitDuration'] > 0]\n",
    "\n",
    "ad_visits = logs_with_duration[\n",
    "    logs_with_duration['ym:s:lastTrafficSource'] == 'ad'\n",
    "]\n",
    "organic_visits = logs_with_duration[\n",
    "    logs_with_duration['ym:s:lastTrafficSource'] == 'organic'\n",
    "]\n",
    "\n",
    "\n",
    "# Histograma 1\n",
    "ad_visits['ym:s:visitDuration'].hist(alpha=0.5, range=(0,2000))\n",
    "# Histograma 2\n",
    "organic_visits['ym:s:visitDuration'].hist(alpha=0.5, range=(0,2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Pon a prueba la hipótesis de que los dos valores promedio son iguales. A continuación, evalúa si la diferencia entre la duración promedio de las visitas de las dos fuentes de tráfico es estadísticamente significativa.\n",
    "Haz alpha (el nivel de significación) 0.05.\n",
    "Si la hipótesis es rechazada, imprime el mensaje: \"Rejecting the null hypothesis\". De lo contrario, imprime \"Failed to reject the null hypothesis\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats as st\n",
    "\n",
    "logs = pd.read_csv('/datasets/analytics_logs.csv')\n",
    "\n",
    "logs_with_duration = logs[logs['ym:s:visitDuration'] > 0]\n",
    "\n",
    "ad_visits = logs_with_duration[\n",
    "    logs_with_duration['ym:s:lastTrafficSource'] == 'ad'\n",
    "]\n",
    "organic_visits = logs_with_duration[\n",
    "    logs_with_duration['ym:s:lastTrafficSource'] == 'organic'\n",
    "]\n",
    "\n",
    "ad_visits_durations = ad_visits['ym:s:visitDuration']\n",
    "organic_visits_durations = organic_visits['ym:s:visitDuration']\n",
    "\n",
    "\n",
    "#Se establecen las hipótesis que se están evaluando de manera explícita:\n",
    "#H_0 Los valores promedio son iguales ó H_1 Los valores promedio son diferentes.\n",
    "#En este caso no se hizo la validaciòn de supuestos de Normalidad (test shapiro-wilk), Homogeneidad de varianzas (test de Levene)\n",
    "print(ad_visits_durations.mean())\n",
    "print(organic_visits_durations.mean())\n",
    "\n",
    "# Establece el criterio de significación estadística\n",
    "alpha = 0.05 \n",
    "# Se realiza la pru\n",
    "results = st.ttest_ind(ad_visits_durations,organic_visits_durations,equal_var=True)\n",
    "\n",
    "# Compara el valor p con el conjunto alfa\n",
    "\n",
    "if results.pvalue < alpha:\n",
    "    print(\"Rejecting the null hypothesis\")\n",
    "else:\n",
    "    print(\"Failed to reject the null hypothesis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
